{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695a150a",
   "metadata": {},
   "source": [
    "# Codtech — Data Analyst Internship\n",
    "\n",
    "**Task 1: Big data analysis**\n",
    "\n",
    "This notebook demonstrates scalable analysis using **Dask** (with a pandas fallback if Dask is unavailable). We use a synthetic dataset (~10,000 rows) that simulates user events (click/view/purchase/etc.). The notebook includes data loading, cleaning, aggregation, and some simple visualizations and insights.\n",
    "\n",
    "Files generated with this notebook:\n",
    "\n",
    "- `codtech_bigdata_sample_10000.csv` — dataset (~10k rows)\n",
    "- `codtech_bigdata_analysis_notebook.ipynb` — this notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import Dask. If unavailable, fall back to pandas and note limitations.\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    dask_available = True\n",
    "    print('Dask is available — will use Dask for scalable operations.')\n",
    "except Exception as e:\n",
    "    dask_available = False\n",
    "    print('Dask not available — falling back to pandas. To run with Dask, install it in your environment (pip install dask[complete]).')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print('pandas version:', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ccc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (using Dask if available)\n",
    "csv_path = r\"/mnt/data/codtech_data_analytics/codtech_bigdata_sample_10000.csv\"\n",
    "if dask_available:\n",
    "    df = dd.read_csv(csv_path, assume_missing=True, parse_dates=['timestamp'])\n",
    "    print('Loaded with Dask dataframe. Row count (lazy):', df.shape[0])\n",
    "else:\n",
    "    df = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "    print('Loaded with pandas DataFrame. Rows:', len(df))\n",
    "\n",
    "# Show a small sample\n",
    "try:\n",
    "    display(df.head())\n",
    "except Exception:\n",
    "    print(df.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning & feature engineering\n",
    "def prepare(df, dask_available):\n",
    "    if dask_available:\n",
    "        # ensure correct dtypes\n",
    "        df['event_type'] = df['event_type'].astype('category')\n",
    "        df['category'] = df['category'].astype('category')\n",
    "        df['country'] = df['country'].astype('category')\n",
    "        df['value'] = df['value'].fillna(0).astype(float)\n",
    "        # extract date parts\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        return df\n",
    "    else:\n",
    "        df['event_type'] = df['event_type'].astype('category')\n",
    "        df['category'] = df['category'].astype('category')\n",
    "        df['country'] = df['country'].astype('category')\n",
    "        df['value'] = df['value'].fillna(0).astype(float)\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        return df\n",
    "\n",
    "df = prepare(df, dask_available)\n",
    "print('Prepared dataframe — showing dtypes:')\n",
    "try:\n",
    "    print(df.dtypes)\n",
    "except Exception:\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29234f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations: event counts, purchases, total revenue by country & category\n",
    "if dask_available:\n",
    "    # event counts per day\n",
    "    daily_events = df.groupby('date')['event_type'].count().compute().rename('events')\n",
    "    # purchases and revenue by country\n",
    "    purchases_country = df[df['event_type']=='purchase'].groupby('country').agg({'event_type':'count','value':'sum'}).compute()\n",
    "    purchases_country = purchases_country.rename(columns={'event_type':'purchase_count','value':'total_revenue'}).sort_values('total_revenue', ascending=False)\n",
    "else:\n",
    "    daily_events = df.groupby('date')['event_type'].count().rename('events')\n",
    "    purchases_country = df[df['event_type']=='purchase'].groupby('country').agg(purchase_count=('event_type','count'), total_revenue=('value','sum')).sort_values('total_revenue', ascending=False)\n",
    "\n",
    "print('Top 5 days by event count:')\n",
    "display(daily_events.sort_values(ascending=False).head())\n",
    "print('\\nPurchases & revenue by country:')\n",
    "display(purchases_country.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-level metrics: views -> purchases conversion estimate\n",
    "if dask_available:\n",
    "    cat_counts = df.groupby(['category','event_type']).size().compute().unstack(fill_value=0)\n",
    "else:\n",
    "    cat_counts = df.groupby(['category','event_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Ensure columns exist\n",
    "for col in ['view','purchase','click','signup','refund']:\n",
    "    if col not in cat_counts.columns:\n",
    "        cat_counts[col] = 0\n",
    "\n",
    "cat_counts['purchase_rate_per_view'] = cat_counts['purchase'] / (cat_counts['view'].replace(0, np.nan))\n",
    "cat_counts = cat_counts.sort_values('purchase_rate_per_view', ascending=False)\n",
    "display(cat_counts[['view','purchase','purchase_rate_per_view']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6814507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visualization: top countries by revenue (matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    if dask_available:\n",
    "        top = purchases_country.compute() if hasattr(purchases_country, 'compute') else purchases_country\n",
    "    else:\n",
    "        top = purchases_country.copy()\n",
    "    top = top.sort_values('total_revenue', ascending=False).head(10)\n",
    "    ax = top['total_revenue'].plot(kind='bar', legend=False)\n",
    "    ax.set_title('Top countries by total revenue')\n",
    "    ax.set_ylabel('Total revenue')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Plot failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d3968",
   "metadata": {},
   "source": [
    "## Quick insights (automatically derived)\n",
    "\n",
    "- The dataset simulates user events with a higher proportion of non-purchase events (clicks/views).\n",
    "- Revenue is concentrated in countries with higher simulated purchase counts. Check the `purchases_country` table for exact figures.\n",
    "- Category-level purchase rates (`purchase_rate_per_view`) highlight which categories convert best from views to purchases.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "- This notebook tries to use **Dask** for scalable operations. If your environment doesn't have Dask installed, the notebook will fall back to pandas (which works for ~10k rows but is not distributed). To scale to truly \"big data\" (millions of rows), run this notebook in an environment with Dask or PySpark installed and adequate compute resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a quick summary CSVs for the main outputs\n",
    "out_dir = r\"/mnt/data/codtech_data_analytics\"\n",
    "if dask_available:\n",
    "    daily_events.to_csv(out_dir + '/daily_events_summary.csv', index=True)\n",
    "    purchases_country.to_csv(out_dir + '/purchases_by_country_summary.csv')\n",
    "else:\n",
    "    daily_events.to_csv(out_dir + '/daily_events_summary.csv', index=True)\n",
    "    purchases_country.to_csv(out_dir + '/purchases_by_country_summary.csv')\n",
    "\n",
    "print('Saved summary CSVs to', out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
